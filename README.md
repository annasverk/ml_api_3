# Flask-приложение для обучения ML-моделей с поддержкой мониторинга, экспериментов и CI
В рамках выполнения Домашнего задания 3 по курсу "Python: продвинутый уровень" было 
усовершенствовано Flask-приложение для обучения ML-моделей из задания 1:
- Добавлено логирование (вывод в sys.stdout)
- Добавлен мониторинг с помощью Prometheus
- Добавлено управление экспериментами с помощью MLflow
- Добавлен CI
    - тесты
    - сборка docker-образов с публикацией в DockerHub
    - сборка в Travis CI (почти)

Приложение можно запустить утилитой docker-compose следующим образом:
```
docker-compose build
docker-compose up
``` 

Для запуска проекта необходимо склонировать репозиторий и перейти в папку проекта,
создать там виртуальное окружение и активировать его, после чего можно запускать docker-compose.


## API

#### С помощью __/ml_api__ можно:
1. Вернуть список доступных для обучения классов моделей
2. Обучить ML-модель с возможностью настройки и подбора оптимальных значений (grid search) гиперпараметров
3. Рассчитать предсказания конкретной модели (система умеет хранить как обученные модели, так и обучающие и тестовые данные)
4. Обучить заново (на новых данных) и удалить уже обученные модели

Всего реализовано 4 класса моделей:
- Линейные модели
- Метод опорных векторов
- Дерево решений
- Случайный лес

На вход приложение принимает данные в формате json, где все признаки являются количественными, а таргету соответствует последний элемент файла.

### Example
Подробный пример использования приложения представлен в ноутбуке [API-example.ipynb](API-example.ipynb).

## Логирование и мониторинг
Логирование происходит во время работы Flask-приложения. В лог пишется:
- все, что связано с появлением исключений
- важные события (чтение данных, разделение на трейн и тест, начало и завершение обучения, расчет метрик качества, сохранение модели и данных)
- время выполнения критичных с точки зрения производительности участков кода (обучение модели, особенно полезно при GridSearchCV с большой сеткой для перебора).

Мониторинг приложения осуществляется посредством Prometheus, который логирует набор дефолтных метрик, а также ряд кастомных, в частности:
1. __by_status_counter__ – счетчик запросов по статусу выполнения (сколько было сделано запросов с тем или иным статусом выполнения)
2. __cnt_get_all_models__ – счетчик __get__ запроса к ручке __/ml_api/all_models__ (сколько раз смотрели словарь всех обученных моделей)
3. __cnt_get_models__ – счетчик __get__ запроса к ручке __/ml_api__ (сколько раз смотрели доступные классы моделей)
4. __cnt_get_preds__ – счетчик __get__ запроса к ручке __/ml_api/<model_name>/<model_version>__ по имени модели и статусу (сколько раз делали предсказания для той или иной модели любой версии с тем или иным статусом выполнения запроса – удалось получить предсказания или была какая-то ошибка)
5. __cnt_deletes__ – счетчик __delete__ запроса к ручке __/ml_api/<model_name>/<model_version>__ по имени модели и статусу (сколько раз удаляли версии той или иной модели с тем или иным статусом выполнения запроса – удалось удалить модель или была какая-то ошибка)
6. __cnt_trains__ – summary (кол-во запросов и время выполнения) __post__ запроса к ручке __/ml_api/<model_name>/<model_version>__ по имени модели и статусу (сколько обучено версий той или иной модели с тем или иным статусом выполнения запроса – удалось обучить модель или была какая-то ошибка)
7. __cnt_estimator_uses__ – summary (кол-во запросов и время выполнения) __post__ запроса к ручке __/ml_api/<model_name>/<model_version>__ по алгоритму и статусу (сколько раз для обучения модели использован той или иной алгоритм (estimator) с тем или иным статусом выполнения запроса – удалось обучить модель или была какая-то ошибка)
8. __cnt_retrains__ – summary (кол-во запросов и время выполнения) __put__ запроса к ручке __/ml_api/<model_name>/<model_version>__ по имени модели и статусу (сколько переобучений было для той или иной модели с тем или иным статусом выполнения запроса – удалось обучить модель на новых данных или была какая-то ошибка)

Метрики собираются каждые 5 секунд, отследить их можно по:
- http://127.0.0.1:8080/metrics – Flask-app
- http://127.0.0.1:9090 – Prometheus
- http://127.0.0.1:3000 – Grafana (логин – admin, пароль – admin)

На последнем ресурсе можно построить красивый дэшборд для визуализации метрик!)

## MLflow
С помощью MLflow в приложении реализована возможность управлять всем жизненным циклом обучения ML модели. 
В частности, можно управлять экспериментами, обучать несколько версий одной модели – с разными гиперпараметрами или на разных выборках.
Благодаря этому теперь можно делать предсказания, переобучать и удалять конкретные версии конкретных моделей. 
Параметры, метрики качества и сама модель сохраняются на стороне MLflow.

Посмотреть результаты экспериментов можно здесь: http://127.0.0.1:5050.

## Тесты
Реализован набор тестов на проверку корректности работы приложения. В частности, проверяются такие аспекты как
ошибки и исключения, качество данных, смещение между трейном и тестом, все этапы моделирования (обучение, предсказание, переобучение, удаление).

Запуск тестов осуществляется командой:
```
python -m unittest discover
``` 
Факт прохождения всех тестов зафиксирован в файле [test_results.txt](test_results.txt).

Также была определена оценка покрытия тестами и качества кода:
```
coverage run -m unittest discover
coverage report
pylint flask_app/app.py
``` 
Покрытие составило 85%, а качество кода – 6.92/10.
Результаты представлены в файлах [coverage_results.txt](coverage_results.txt) и [pylint_results.txt](pylint_results.txt) соответственно.

## Docker
Получившееся приложение собрано в Docker-образы и они опубликованы в DockerHub:
- app: https://hub.docker.com/repository/docker/annasverk/hw3_app
- mlflow: https://hub.docker.com/repository/docker/annasverk/hw3_mlflow

Для Prometheus и Grafana использовались уже готовые образы из DockerHub.

Приложение можно запустить утилитой docker-compose. Все контейнеры поднимаются, 
что проверено в приложении Docker Desktop.

## TravisCI
Очень сильно пыталась запустить все через TravisCI, запустила там docker-compose, но не получилось поднять все целиком из-за MLflow.
Поэтому значки о прохождении тестов добавить не удалось... Тем не менее, все тесты пройдены успешно!

P.s. Кстати, если поднимать без тестов, то сборка удавалась (был зеленый значок build passing).